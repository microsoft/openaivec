{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Async Processing with `.aio`\n",
    "\n",
    "This notebook explains `.aio.responses` in three steps: default usage, batch-size tuning, and parallelism tuning.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup\n",
    "\n",
    "Configure authentication first, then set the default responses model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from openaivec import pandas_ext\n",
    "\n",
    "assert os.getenv(\"OPENAI_API_KEY\") or os.getenv(\"AZURE_OPENAI_BASE_URL\"), (\n",
    "    \"Set OPENAI_API_KEY or Azure OpenAI environment variables before running this notebook.\"\n",
    ")\n",
    "\n",
    "pandas_ext.set_responses_model(\"gpt-5.1\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This product is amazing!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Terrible customer service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Good value for money</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Not what I expected</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>This product is amazing!</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        text\n",
       "0   This product is amazing!\n",
       "1  Terrible customer service\n",
       "2       Good value for money\n",
       "3        Not what I expected\n",
       "4   This product is amazing!"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    \"text\": [\n",
    "        \"This product is amazing!\",\n",
    "        \"Terrible customer service\",\n",
    "        \"Good value for money\",\n",
    "        \"Not what I expected\",\n",
    "    ]\n",
    "    * 25\n",
    "})\n",
    "\n",
    "prompt = \"Analyze sentiment and classify as positive/negative/neutral\"\n",
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Default example (no tuning)\n",
    "\n",
    "Start with no extra parameters. This uses the library defaults.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "32045f77e40d4353bfd7a1db5e014ae5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/4 [00:00<?, ?item/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    Sentiment: positive\n",
       "1    Sentiment: negative\n",
       "2    Sentiment: positive\n",
       "3    Sentiment: negative\n",
       "4    Sentiment: positive\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_sentiments = await df[\"text\"].aio.responses(prompt)\n",
    "default_sentiments.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Batch-size example\n",
    "\n",
    "`batch_size` controls how many inputs are grouped into one API call.\n",
    "Larger values reduce request overhead, while smaller values can be safer for unstable workloads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c5ca836af1e4d8f8226347799e1ef26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/4 [00:00<?, ?item/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    Sentiment: positive\n",
       "1    Sentiment: negative\n",
       "2    Sentiment: positive\n",
       "3    Sentiment: negative\n",
       "4    Sentiment: positive\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size_sentiments = await df[\"text\"].aio.responses(\n",
    "    prompt,\n",
    "    batch_size=32,\n",
    ")\n",
    "batch_size_sentiments.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Parallelism example\n",
    "\n",
    "`max_concurrency` controls how many batches run in parallel.\n",
    "Increase gradually to improve throughput while staying within API rate limits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f514b27b9a84879b6445a3bcb7a7234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing batches:   0%|          | 0/4 [00:00<?, ?item/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0    Sentiment: positive\n",
       "1    Sentiment: negative\n",
       "2    Sentiment: positive\n",
       "3    Sentiment: negative\n",
       "4    Sentiment: positive\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_sentiments = await df[\"text\"].aio.responses(\n",
    "    prompt,\n",
    "    batch_size=32,\n",
    "    max_concurrency=12,\n",
    ")\n",
    "parallel_sentiments.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary\n",
    "\n",
    "Use this order when tuning `.aio.responses`:\n",
    "\n",
    "1. Start with defaults (no extra parameters).\n",
    "2. Set `batch_size` first (`16` to `64` is a practical starting range).\n",
    "3. Then increase `max_concurrency` (`4` to `12`) while watching rate limits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Goal | Suggested Setting |\n",
    "| --- | --- |\n",
    "| Keep it simple | default (`batch_size=None`, default concurrency) |\n",
    "| Reduce request overhead | increase `batch_size` |\n",
    "| Improve throughput | increase `max_concurrency` gradually |\n",
    "| Stay stable under limits | lower both values |\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
